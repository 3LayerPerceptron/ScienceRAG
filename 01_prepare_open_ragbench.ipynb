{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Подготовка датасета Open RAG Benchmark",
   "id": "c62f04ca0077a855"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tre_i\\PycharmProjects\\ScienceRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json, os"
   ],
   "id": "initial_id"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T01:26:09.695832Z",
     "start_time": "2025-12-05T01:26:09.689570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "CHUNK_MAX_CHARS = 1800   # примерный размер чанка\n",
    "CHUNK_MIN_CHARS = 600\n",
    "CHUNK_OVERLAP_CHARS = 200"
   ],
   "id": "596520e92435ee65",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создание папок",
   "id": "108b58b4c9a9a267"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T01:34:25.679190Z",
     "start_time": "2025-12-05T01:34:25.673164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import list_repo_files, hf_hub_download\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "DATA_ROOT = Path(\"data\")\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "CORPUS_DIR = RAW_DIR / \"corpus\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CORPUS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Folders ready:\", RAW_DIR, CORPUS_DIR)"
   ],
   "id": "1100fb49916a292c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders ready: data\\raw data\\raw\\corpus\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Смотрим на файлы датасета https://huggingface.co/datasets/vectara/open_ragbench",
   "id": "cbaf5c5c71e39ba2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T01:34:38.734004Z",
     "start_time": "2025-12-05T01:34:38.290019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "files = list_repo_files(\"vectara/open_ragbench\", repo_type=\"dataset\")\n",
    "\n",
    "print(\"Total files:\", len(files))\n",
    "for f in files[:20]:   # показать первые 20\n",
    "    print(f)"
   ],
   "id": "d16661c25c22df95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 1006\n",
      ".gitattributes\n",
      "README.md\n",
      "pdf/arxiv/answers.json\n",
      "pdf/arxiv/corpus/2401.01872v2.json\n",
      "pdf/arxiv/corpus/2401.02247v4.json\n",
      "pdf/arxiv/corpus/2401.02564v2.json\n",
      "pdf/arxiv/corpus/2401.03126v3.json\n",
      "pdf/arxiv/corpus/2401.03305v2.json\n",
      "pdf/arxiv/corpus/2401.03328v2.json\n",
      "pdf/arxiv/corpus/2401.03345v2.json\n",
      "pdf/arxiv/corpus/2401.03776v8.json\n",
      "pdf/arxiv/corpus/2401.05657v5.json\n",
      "pdf/arxiv/corpus/2401.05762v4.json\n",
      "pdf/arxiv/corpus/2401.05851v4.json\n",
      "pdf/arxiv/corpus/2401.06326v4.json\n",
      "pdf/arxiv/corpus/2401.06740v2.json\n",
      "pdf/arxiv/corpus/2401.06867v3.json\n",
      "pdf/arxiv/corpus/2401.06959v2.json\n",
      "pdf/arxiv/corpus/2401.06987v2.json\n",
      "pdf/arxiv/corpus/2401.07152v3.json\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Скачивание файлов с метаданными и Q&A",
   "id": "cc2c82cf1a011775"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T01:35:08.654997Z",
     "start_time": "2025-12-05T01:35:07.864649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "META_DIR = RAW_DIR\n",
    "\n",
    "def download_file(path_in_repo, dest_dir=META_DIR):\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=\"vectara/open_ragbench\",\n",
    "        filename=path_in_repo,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    dest = dest_dir / Path(path_in_repo).name\n",
    "    dest.write_bytes(Path(local_path).read_bytes())\n",
    "    print(\"Saved:\", dest)\n",
    "    return dest\n",
    "\n",
    "# Находим все meta-файлы\n",
    "meta_files = [f for f in files if f.endswith(\".json\") and \"/corpus/\" not in f]\n",
    "\n",
    "print(\"Meta files:\", meta_files)\n",
    "\n",
    "# Скачиваем\n",
    "downloaded_meta_paths = []\n",
    "for f in meta_files:\n",
    "    downloaded_meta_paths.append(download_file(f, META_DIR))\n"
   ],
   "id": "173118aac9031d28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta files: ['pdf/arxiv/answers.json', 'pdf/arxiv/pdf_urls.json', 'pdf/arxiv/qrels.json', 'pdf/arxiv/queries.json']\n",
      "Saved: data\\raw\\answers.json\n",
      "Saved: data\\raw\\pdf_urls.json\n",
      "Saved: data\\raw\\qrels.json\n",
      "Saved: data\\raw\\queries.json\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Скачивание файлов статей",
   "id": "c6fea4bcd426e0de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T01:39:28.770615Z",
     "start_time": "2025-12-05T01:36:17.457177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ищем все файлы в папке corpus/\n",
    "corpus_files = [f for f in files if \"/corpus/\" in f]\n",
    "\n",
    "print(\"Corpus files:\", len(corpus_files))\n",
    "\n",
    "for f in tqdm(corpus_files):\n",
    "    local_path = hf_hub_download(\n",
    "        repo_id=\"vectara/open_ragbench\",\n",
    "        filename=f,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    dest = CORPUS_DIR / Path(f).name\n",
    "    dest.write_bytes(Path(local_path).read_bytes())"
   ],
   "id": "c8f0ba8784f5dd9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus files: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [03:11<00:00,  5.23it/s]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Пример текста",
   "id": "cd63449c5c2e2bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T01:41:34.364570Z",
     "start_time": "2025-12-05T01:41:34.348272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example_doc = next(CORPUS_DIR.glob(\"*.json\"))\n",
    "\n",
    "with open(example_doc, \"r\", encoding=\"utf-8\") as f:\n",
    "    doc = json.load(f)\n",
    "\n",
    "print(\"Document keys:\", doc.keys())\n",
    "print(\"Title:\", doc.get(\"title\"))\n",
    "print(\"Number of sections:\", len(doc.get(\"sections\", [])))\n",
    "print(doc[\"sections\"][0][\"text\"][:500])"
   ],
   "id": "20ac8d6e0d9c0d98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document keys: dict_keys(['title', 'sections', 'id', 'authors', 'categories', 'abstract', 'updated', 'published'])\n",
      "Title: Multiple Imputation of Hierarchical Nonlinear Time Series Data with an\n",
      "  Application to School Enrollment Data\n",
      "Number of sections: 9\n",
      "#### Abstract\n",
      "\n",
      "International comparisons of hierarchical time series data sets based on survey data, such as annual country-level estimates of school enrollment rates, can suffer from large amounts of missing data due to differing coverage of surveys across countries and across times. A popular approach to handling missing data in these settings is through multiple imputation, which can be especially effective when there is an auxiliary variable that is strongly predictive of and has a smaller a\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T01:44:47.281365Z",
     "start_time": "2025-12-05T01:44:47.260966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"data\")\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "CORPUS_DIR = RAW_DIR / \"corpus\"\n",
    "\n",
    "PROCESSED_DIR = DATA_ROOT / \"processed\"\n",
    "SAMPLES_DIR = DATA_ROOT / \"samples\"\n",
    "BENCHMARK_DIR = DATA_ROOT / \"benchmark\"\n",
    "\n",
    "for d in [PROCESSED_DIR, SAMPLES_DIR, BENCHMARK_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Processed:\", PROCESSED_DIR)\n",
    "print(\"Samples:\", SAMPLES_DIR)"
   ],
   "id": "cb72eee570b9338f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: data\\processed\n",
      "Samples: data\\samples\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Функция для чанкования",
   "id": "9ebbd63fbd7c377e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T02:38:23.297577Z",
     "start_time": "2025-12-05T02:38:23.285051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    max_chars: int = 1800,\n",
    "    min_chars: int = 800,\n",
    "    overlap_chars: int = 200,\n",
    "):\n",
    "    \"\"\"\n",
    "    Балансированный чанкер для научного текста с LaTeX:\n",
    "    - предпочитает резать по абзацам (\\n\\n),\n",
    "    - дальше по границе предложения (. ? !),\n",
    "    - дальше по пробелу,\n",
    "    - старается не резать внутри формул ($...$, $$...$$),\n",
    "    - использует overlap, но начало чанка сдвигает к границе слова,\n",
    "    - не зацикливается.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    text = text.strip()\n",
    "    n = len(text)\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    sentence_punct = \".?!。！？\"\n",
    "\n",
    "    def is_inside_math(pos: int) -> bool:\n",
    "        \"\"\"Эвристика: нечётное количество $ до позиции → внутри формулы.\"\"\"\n",
    "        before = text[:pos]\n",
    "        dollar_count = 0\n",
    "        i = 0\n",
    "        while i < len(before):\n",
    "            if before[i] == \"\\\\\":\n",
    "                i += 2  # пропускаем экранированные символы\n",
    "                continue\n",
    "            if before[i] == \"$\":\n",
    "                dollar_count += 1\n",
    "            i += 1\n",
    "        return dollar_count % 2 == 1\n",
    "\n",
    "    while start < n:\n",
    "        # Хвост меньше max_chars — забираем целиком\n",
    "        if n - start <= max_chars:\n",
    "            chunk = text[start:].strip()\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "            break\n",
    "\n",
    "        window_end = min(n, start + max_chars)\n",
    "        search_from = start + min_chars\n",
    "        if search_from >= window_end:\n",
    "            search_from = start\n",
    "\n",
    "        split = None\n",
    "\n",
    "        # 1) Абзацный разрыв \\n\\n\n",
    "        para_pos = text.rfind(\"\\n\\n\", search_from, window_end)\n",
    "        if para_pos != -1 and para_pos > start:\n",
    "            candidate = para_pos + 2\n",
    "            if not is_inside_math(candidate):\n",
    "                split = candidate\n",
    "\n",
    "        # 2) Граница предложения\n",
    "        if split is None:\n",
    "            candidate = -1\n",
    "            for i in range(window_end - 1, search_from - 1, -1):\n",
    "                ch = text[i]\n",
    "                if ch in sentence_punct:\n",
    "                    j = i + 1\n",
    "                    if j >= n or text[j].isspace():\n",
    "                        candidate = i + 1\n",
    "                        break\n",
    "            if candidate != -1 and candidate > start and not is_inside_math(candidate):\n",
    "                split = candidate\n",
    "\n",
    "        # 3) Пробел\n",
    "        if split is None:\n",
    "            candidate = text.rfind(\" \", search_from, window_end)\n",
    "            if candidate != -1 and candidate > start:\n",
    "                split = candidate\n",
    "\n",
    "        # 4) Жёсткий разрез\n",
    "        if split is None or split <= start:\n",
    "            split = window_end\n",
    "\n",
    "        # Сам чанк\n",
    "        chunk = text[start:split].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "\n",
    "        # ==== новый start с оверлапом, БЕЗ разрезания слов ====\n",
    "\n",
    "        if overlap_chars <= 0:\n",
    "            next_start = split\n",
    "        else:\n",
    "            raw_next = max(split - overlap_chars, 0)\n",
    "\n",
    "            # Ищем ближайшую границу слова между raw_next и split:\n",
    "            # сначала пробел/перенос вперёд от raw_next\n",
    "            boundary = -1\n",
    "            for i in range(raw_next, split):\n",
    "                if text[i].isspace():\n",
    "                    boundary = i + 1  # начало следующего слова\n",
    "                    break\n",
    "\n",
    "            if boundary != -1:\n",
    "                next_start = boundary\n",
    "            else:\n",
    "                # если внутри overlap нет пробелов — начинаем с split\n",
    "                next_start = split\n",
    "\n",
    "        # Пропускаем начальные пробелы/переводы строк\n",
    "        while next_start < n and text[next_start].isspace():\n",
    "            next_start += 1\n",
    "\n",
    "        # Гарантируем движение вперёд\n",
    "        if next_start <= start:\n",
    "            next_start = start + max(1, max_chars // 2)\n",
    "\n",
    "        start = next_start\n",
    "\n",
    "    return chunks\n",
    "\n"
   ],
   "id": "4379cd565954a1c8",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Создание чанков",
   "id": "bb232e628a2a49ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T02:38:54.249300Z",
     "start_time": "2025-12-05T02:38:30.335737Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "chunk_records = []\n",
    "\n",
    "corpus_files = sorted(CORPUS_DIR.glob(\"*.json\"))\n",
    "\n",
    "print(\"Corpus files:\", len(corpus_files))\n",
    "\n",
    "for doc_path in tqdm(corpus_files, desc=\"Chunking corpus\"):\n",
    "    with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = json.load(f)\n",
    "\n",
    "    doc_id = doc.get(\"id\") or doc.get(\"doc_id\") or doc_path.stem\n",
    "    title = doc.get(\"title\", \"\")\n",
    "\n",
    "    sections = doc.get(\"sections\") or []\n",
    "    if not sections and \"text\" in doc:\n",
    "        # fallback: весь текст одной секцией\n",
    "        sections = [{\"heading\": \"\", \"text\": doc[\"text\"]}]\n",
    "\n",
    "    for sec_idx, sec in enumerate(sections):\n",
    "        sec_heading = sec.get(\"heading\") or sec.get(\"title\") or \"\"\n",
    "        sec_text = sec.get(\"text\") or \"\"\n",
    "\n",
    "        if not sec_text.strip():\n",
    "            continue\n",
    "\n",
    "        chunks = chunk_text(sec_text)\n",
    "        for ch_idx, ch in enumerate(chunks):\n",
    "            chunk_records.append({\n",
    "                \"chunk_id\": f\"{doc_id}_sec{sec_idx}_chunk{ch_idx}\",\n",
    "                \"doc_id\": doc_id,\n",
    "                \"section_index\": sec_idx,\n",
    "                \"chunk_index\": ch_idx,\n",
    "                \"title\": title,\n",
    "                \"section_heading\": sec_heading,\n",
    "                \"text\": ch,\n",
    "            })\n",
    "\n",
    "print(\"Total chunks:\", len(chunk_records))"
   ],
   "id": "9ae1882c30263a18",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus files: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking corpus: 100%|██████████| 1000/1000 [00:23<00:00, 41.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 66949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Запись чанков (всех и примеров) на локальный диск",
   "id": "35f7e3660e78ed36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T02:40:02.936385Z",
     "start_time": "2025-12-05T02:40:02.125593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"data\")\n",
    "PROCESSED_DIR = DATA_ROOT / \"processed\"\n",
    "SAMPLES_DIR = DATA_ROOT / \"samples\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "chunks_path = PROCESSED_DIR / \"chunks.jsonl\"\n",
    "sample_path = SAMPLES_DIR / \"chunks_sample.jsonl\"\n",
    "\n",
    "def save_jsonl(records, path: Path, limit: int | None = None):\n",
    "    n = len(records) if limit is None else min(len(records), limit)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in range(n):\n",
    "            f.write(json.dumps(records[i], ensure_ascii=False) + \"\\n\")\n",
    "    print(f\"Saved {n} records to {path}\")\n",
    "\n",
    "save_jsonl(chunk_records, chunks_path)          # все 60k\n",
    "save_jsonl(chunk_records, sample_path, 50)      # сэмпл для GitHub/README"
   ],
   "id": "805b7d4b141f6b6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 66949 records to data\\processed\\chunks.jsonl\n",
      "Saved 50 records to data\\samples\\chunks_sample.jsonl\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T02:36:00.312006Z",
     "start_time": "2025-12-05T02:36:00.304625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def mb(path: Path) -> float:\n",
    "    return path.stat().st_size / (1024 * 1024)\n",
    "\n",
    "print(\"Total chunks:\", len(chunk_records))\n",
    "print(\"chunks.jsonl size:\", f\"{mb(chunks_path):.2f} MB\")"
   ],
   "id": "9208135d54417d72",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 67206\n",
      "chunks.jsonl size: 107.41 MB\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T02:41:10.693848Z",
     "start_time": "2025-12-05T02:41:10.481758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# загружаем ключи из .env\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token is None:\n",
    "    raise ValueError(\"HF_TOKEN not found in .env\")\n",
    "\n",
    "# 1. логин в HuggingFace\n",
    "login(token=hf_token)\n",
    "\n",
    "print(\"Logged in to HuggingFace!\")"
   ],
   "id": "f923828cea77b32e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in to HuggingFace!\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Пушим чанки в качестве датасета на HuggingfaceHub",
   "id": "1c0ae978feb25dc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-05T02:41:23.556248Z",
     "start_time": "2025-12-05T02:41:12.582247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "# загружаем чанки\n",
    "processed_chunks = []\n",
    "with open(\"data/processed/chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        processed_chunks.append(json.loads(line))\n",
    "\n",
    "chunks_ds = Dataset.from_list(processed_chunks)\n",
    "\n",
    "# пушим только корпус\n",
    "chunks_ds.push_to_hub(\"Ilya-huggingface/open_ragbench_chunks\")"
   ],
   "id": "108e1e0a5df70e1a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]\n",
      "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]\u001B[A\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00,  5.69ba/s]\u001B[A\n",
      "Processing Files (0 / 0): |          |  0.00B /  0.00B            \n",
      "Processing Files (0 / 1):   1%|▏         |  526kB / 39.9MB,  527kB/s  \n",
      "Processing Files (0 / 1):   3%|▎         | 1.05MB / 39.9MB,  876kB/s  \n",
      "Processing Files (0 / 1):   5%|▌         | 2.10MB / 39.9MB, 1.50MB/s  \n",
      "Processing Files (0 / 1):   9%|▉         | 3.68MB / 39.9MB, 2.30MB/s  \n",
      "Processing Files (0 / 1):  20%|█▉        | 7.89MB / 39.9MB, 4.39MB/s  \n",
      "Processing Files (0 / 1):  25%|██▌       | 9.99MB / 39.9MB, 5.00MB/s  \n",
      "Processing Files (0 / 1):  30%|███       | 12.1MB / 39.9MB, 5.50MB/s  \n",
      "Processing Files (0 / 1):  34%|███▍      | 13.7MB / 39.9MB, 5.70MB/s  \n",
      "Processing Files (0 / 1):  41%|████      | 16.3MB / 39.9MB, 6.28MB/s  \n",
      "Processing Files (0 / 1):  44%|████▎     | 17.4MB / 39.9MB, 6.20MB/s  \n",
      "Processing Files (0 / 1):  48%|████▊     | 18.9MB / 39.9MB, 6.32MB/s  \n",
      "Processing Files (0 / 1):  53%|█████▎    | 21.0MB / 39.9MB, 6.58MB/s  \n",
      "Processing Files (0 / 1):  59%|█████▉    | 23.7MB / 39.9MB, 6.97MB/s  \n",
      "Processing Files (0 / 1):  65%|██████▍   | 25.8MB / 39.9MB, 7.16MB/s  \n",
      "Processing Files (0 / 1):  71%|███████▏  | 28.4MB / 39.9MB, 7.48MB/s  \n",
      "Processing Files (0 / 1):  77%|███████▋  | 30.5MB / 39.9MB, 7.63MB/s  \n",
      "Processing Files (0 / 1):  82%|████████▏ | 32.6MB / 39.9MB, 7.77MB/s  \n",
      "Processing Files (0 / 1):  87%|████████▋ | 34.7MB / 39.9MB, 7.89MB/s  \n",
      "Processing Files (0 / 1):  94%|█████████▎| 37.3MB / 39.9MB, 8.12MB/s  \n",
      "Processing Files (0 / 1):  99%|█████████▉| 39.4MB / 39.9MB, 8.22MB/s  \n",
      "Processing Files (1 / 1): 100%|██████████| 39.9MB / 39.9MB, 6.87MB/s  \n",
      "Processing Files (1 / 1): 100%|██████████| 39.9MB / 39.9MB, 6.64MB/s  \n",
      "New Data Upload: 100%|██████████| 39.9MB / 39.9MB, 6.64MB/s  \n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:07<00:00,  7.40s/ shards]\n",
      "C:\\Users\\tre_i\\PycharmProjects\\ScienceRAG\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:121: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\tre_i\\.cache\\huggingface\\hub\\datasets--Ilya-huggingface--open_ragbench_chunks. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/Ilya-huggingface/open_ragbench_chunks/commit/d62422bf18dacab2788306ef5953a8966baac85b', commit_message='Upload dataset', commit_description='', oid='d62422bf18dacab2788306ef5953a8966baac85b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/Ilya-huggingface/open_ragbench_chunks', endpoint='https://huggingface.co', repo_type='dataset', repo_id='Ilya-huggingface/open_ragbench_chunks'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
